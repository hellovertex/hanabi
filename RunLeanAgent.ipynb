{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from learning import learn_pool\n",
    "\n",
    "'''rewards_config = {'play0' : 1, 'play1' : 3, 'play2' : 9, 'play3' : 27, 'play4' : 81,\n",
    "                  'baseline' :3,\n",
    "                  'discard_last_copy' : -100, 'discard_extra' : 0.5,\n",
    "                  'hint_last_copy' : 0.2, 'hint_penalty' : 0.1,  'hint_playable' : 0.2,\n",
    "                  'use_hamming' : True, 'loose_life' : -50}'''\n",
    "\n",
    "##########################################################################\n",
    "env_config = {'environment_name' : 'Hanabi-Small', 'num_players' : 2, \n",
    "              'use_custom_rewards' : False, 'open_hands' : False}\n",
    "rewards_config = {}\n",
    "model_config_base = {'scope' : 'agent_test', 'nenvs'  : 32, \n",
    "           'fc_input_layers' : [128, ], 'noisy_fc' : False, 'v_net' : 'copy',\n",
    "           'gamma' : 0.99, 'ent_coef' : 0.0, 'vf_coef' : 1, 'cliprange' : 0.2,\n",
    "           'max_grad_norm' : None, 'k' : 8,  \n",
    "           'lr' : 1e-4, 'lr_half_period' : 50000, 'anneal_lr' : False,\n",
    "           'normalize_advs': True, 'layer_norm' : False,\n",
    "           }\n",
    "\n",
    "\n",
    "method = 'self play'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "def plot_log(folder, runname,):\n",
    "    agentscope = os.listdir(folder + runname)\n",
    "    path  = folder + runname + '/' + agentscope + '/summary/'\n",
    "    log = os.listdir(path)[0]\n",
    "    for val in tf.train.summary_iterator(path + log):\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO EXP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/learning/learn_pool.py:100: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/learning/learn_pool.py:101: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "GAME PARAMETERS: \n",
      "  observation length = 171\n",
      "  number of actions = 11\n",
      "  number of players = 2\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:43: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:53: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:68: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/util.py:112: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/gr1/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/util.py:81: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/network.py:68: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:95: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:104: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:111: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:111: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:111: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:112: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/PPO/ppo.py:115: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Could not load model \"agent_test\" at ./experiments/test/Hanabi-Small-2/test/agent_test/model/\n",
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/learning/learn_pool.py:142: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gr1/Documents/hanabi/learning/learn_pool.py:142: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Savinig models after 100 epochs of training\n",
      "Average env speed is 2240 ts/second\n",
      "\n",
      "Savinig models after 200 epochs of training\n",
      "Average env speed is 2252 ts/second\n",
      "---------------200---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    0.896875\n",
      "---------------------------------\n",
      "\n",
      "Savinig models after 300 epochs of training\n",
      "Average env speed is 2335 ts/second\n",
      "\n",
      "Savinig models after 400 epochs of training\n",
      "Average env speed is 2100 ts/second\n",
      "---------------400---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test         1.0\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "folder = './experiments/baselines/noexploration/'\n",
    "model_config_base = {'scope' : 'agent_test', 'nenvs'  : 32, \n",
    "           'fc_input_layers' : [128, ], 'noisy_fc' : False, 'v_net' : 'copy',\n",
    "           'gamma' : 0.99, 'ent_coef' : 0.0, 'vf_coef' : 1, 'cliprange' : 0.2,\n",
    "           'max_grad_norm' : None, 'k' : 8,  \n",
    "           'lr' : 1e-4, 'lr_half_period' : 50000, 'anneal_lr' : False,\n",
    "           'normalize_advs': True, 'layer_norm' : False,\n",
    "           }\n",
    "\n",
    "folder = './experiments/test/'\n",
    "\n",
    "learn_pool.run_experiment('test', [model_config_base], env_config, rewards_config, 500, \n",
    "                          folder = folder, method = method, nsteps = 8,)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.client.session.Session"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.ops.reset_default_graph()>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shared copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3e3a1b750eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_net'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mrunname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s_k:%d_st:%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v_net' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = './experiments/baselines/vnet/'\n",
    "k = 24\n",
    "steps = 8\n",
    "epochs = 20000\n",
    "\n",
    "for vnet in ['shared', 'copy']:\n",
    "    mc = dict(model_config_base)\n",
    "    mc['k'] = k\n",
    "    mc['v_net'] = vnet\n",
    "    runname = '%s_k:%d_st:%s' % (v_net, ent, k, steps)\n",
    "    print(runname)\n",
    "    print(mc)\n",
    "        \n",
    "    learn_pool.run_experiment(runname, [mc], env_config, rewards_config, epochs, \n",
    "                              folder = folder, method = method, nsteps = steps,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './experiments/baselines/exploration/'\n",
    "\n",
    "k, steps, epochs, v_net = best\n",
    "\n",
    "for ent in [0.01, 0.05]:\n",
    "    mc = dict(model_config_base)\n",
    "    mc['k'] = k\n",
    "    mc['ent_coef'] = ent\n",
    "    mc['noisy_fc'] = False\n",
    "    runname = 'ent%.2f_k:%d_st:%s' % (ent, k, steps)\n",
    "    print(runname)\n",
    "    print(mc)\n",
    "        \n",
    "    #learn_pool.run_experiment(runname, [mc], env_config, rewards_config, epochs, \n",
    "    #                              folder = folder, method = method, nsteps = steps,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOISY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './experiments/baselines/exploration/'\n",
    "\n",
    "k, steps, epochs, v_net = best\n",
    "\n",
    "mc = dict(model_config_base)\n",
    "mc['k'] = k\n",
    "mc['ent_coef'] = 0\n",
    "mc['noisy_fc'] = True\n",
    "runname = 'noisy.2f_k:%d_st:%s' % (ent, k, steps)\n",
    "print(runname)\n",
    "print(mc)\n",
    "        \n",
    "#learn_pool.run_experiment(runname, [mc], env_config, rewards_config, epochs, \n",
    "#                              folder = folder, method = method, nsteps = steps,)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fc or lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, steps, epochs, v_net, ent, noisy = best\n",
    "\n",
    "for fc, lstm in [([128], []),\n",
    "                 ([128, 128], []),\n",
    "                 ([128], [128])\n",
    "                 ([128], [128, 128])]:\n",
    "    \n",
    "    \n",
    "    mc = dict(model_config_base)\n",
    "    mc['k'] = k\n",
    "    mc['ent_coef'] = ent\n",
    "    mc['noisy_fc'] = noisy\n",
    "    \n",
    "    runname = 'fc' + 'x'.join(fc) +  '_lstm' + 'x'.join(lstm) + 'k:' + str(k) + 'st:' % str(steps)\n",
    "    if noisy:\n",
    "        runname += '_noisy'\n",
    "    else:\n",
    "        runname += 'ent:' + str(ent)\n",
    "    print(runname)\n",
    "    print(mc)\n",
    "        \n",
    "    #learn_pool.run_experiment(runname, [mc], env_config, rewards_config, epochs, \n",
    "    #                              folder = folder, method = method, nsteps = steps,)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### noisy VS exploration for shared and copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME PARAMETERS: \n",
      "  observation length = 171\n",
      "  number of actions = 11\n",
      "  number of players = 2\n",
      "Successfully loaded model \"agent_test\":\n",
      "  \"agent_test\" was trained for 1900 epochs, using 4599744 timesteps\n",
      "  \"agent_test\" has following parameters: k = 8, cliprange = 0.2, lr = 0.000100\n",
      "  \"agent_test\" has following rewards_config: {}\n",
      "\n",
      "Savinig models after 100 epochs of training\n",
      "Average env speed is 3684 ts/second\n",
      "\n",
      "Savinig models after 200 epochs of training\n",
      "Average env speed is 3649 ts/second\n",
      "---------------200---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test     1.76875\n",
      "---------------------------------\n",
      "\n",
      "Savinig models after 300 epochs of training\n",
      "Average env speed is 3756 ts/second\n",
      "\n",
      "Savinig models after 400 epochs of training\n",
      "Average env speed is 3572 ts/second\n",
      "---------------400---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test     1.75625\n",
      "---------------------------------\n",
      "\n",
      "Savinig models after 500 epochs of training\n",
      "Average env speed is 3829 ts/second\n",
      "\n",
      "Savinig models after 600 epochs of training\n",
      "Average env speed is 3863 ts/second\n",
      "---------------600---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test     1.79375\n",
      "---------------------------------\n",
      "\n",
      "Savinig models after 700 epochs of training\n",
      "Average env speed is 3907 ts/second\n",
      "\n",
      "Savinig models after 800 epochs of training\n",
      "Average env speed is 3584 ts/second\n",
      "---------------800---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    1.746875\n",
      "---------------------------------\n",
      "\n",
      "Savinig models after 900 epochs of training\n",
      "Average env speed is 3715 ts/second\n",
      "\n",
      "Savinig models after 1000 epochs of training\n",
      "Average env speed is 3504 ts/second\n",
      "---------------1000---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    1.734375\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 1100 epochs of training\n",
      "Average env speed is 3550 ts/second\n",
      "\n",
      "Savinig models after 1200 epochs of training\n",
      "Average env speed is 3663 ts/second\n",
      "---------------1200---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    1.803125\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 1300 epochs of training\n",
      "Average env speed is 3665 ts/second\n",
      "\n",
      "Savinig models after 1400 epochs of training\n",
      "Average env speed is 3639 ts/second\n",
      "---------------1400---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test      1.7875\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 1500 epochs of training\n",
      "Average env speed is 3669 ts/second\n",
      "\n",
      "Savinig models after 1600 epochs of training\n",
      "Average env speed is 3684 ts/second\n",
      "---------------1600---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test     1.81875\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 1700 epochs of training\n",
      "Average env speed is 3526 ts/second\n",
      "\n",
      "Savinig models after 1800 epochs of training\n",
      "Average env speed is 3765 ts/second\n",
      "---------------1800---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test         1.8\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 1900 epochs of training\n",
      "Average env speed is 3820 ts/second\n",
      "\n",
      "Savinig models after 2000 epochs of training\n",
      "Average env speed is 3648 ts/second\n",
      "---------------2000---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test     1.85625\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 2100 epochs of training\n",
      "Average env speed is 3763 ts/second\n",
      "\n",
      "Savinig models after 2200 epochs of training\n",
      "Average env speed is 3905 ts/second\n",
      "---------------2200---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    1.896875\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 2300 epochs of training\n",
      "Average env speed is 3685 ts/second\n",
      "\n",
      "Savinig models after 2400 epochs of training\n",
      "Average env speed is 3708 ts/second\n",
      "---------------2400---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test        1.95\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 2500 epochs of training\n",
      "Average env speed is 3678 ts/second\n",
      "\n",
      "Savinig models after 2600 epochs of training\n",
      "Average env speed is 3673 ts/second\n",
      "---------------2600---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    1.921875\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 2700 epochs of training\n",
      "Average env speed is 3939 ts/second\n",
      "\n",
      "Savinig models after 2800 epochs of training\n",
      "Average env speed is 3623 ts/second\n",
      "---------------2800---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test     1.93125\n",
      "----------------------------------\n",
      "\n",
      "Savinig models after 2900 epochs of training\n",
      "Average env speed is 3783 ts/second\n",
      "\n",
      "Savinig models after 3000 epochs of training\n",
      "Average env speed is 3854 ts/second\n",
      "---------------3000---------------\n",
      "Matrix of models performance with each others:\n",
      "            agent_test\n",
      "agent_test    1.965625\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-01462fc048bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrunname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fc:128_%s_k:%d_noisy_st:36'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m learn_pool.run_experiment(runname, [mc], env_config, rewards_config, 5000, \n\u001b[0;32m----> 5\u001b[0;31m                           folder = folder, method = method, nsteps = steps,)\n\u001b[0m",
      "\u001b[0;32m~/Documents/HLE github/hanabi/learning/learn_pool.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(run_name, models_configs, env_config, rewards_configs, nupdates, episodes, nsteps, change_lr, save_every, summary_every, evaluation_every, eval_eps, folder, method)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mgame\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnplayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnenvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_env_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# make savers, summaries, history buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/learning/learn_pool.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(game, models, history_buffers, episodes, nsteps, method, epochs_for_ma)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'self play'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_buffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/game/game.py\u001b[0m in \u001b[0;36mplay_untill_train\u001b[0;34m(self, nepisodes, nsteps, noisescale)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0msteps_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnsteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_done\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/game/game.py\u001b[0m in \u001b[0;36mplay_turn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_player\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_timestep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    172\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpromise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpromise\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stack_time_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36m_stack_time_steps\u001b[0;34m(self, time_steps)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m       return nest_utils.fast_map_structure(\n\u001b[0;32m--> 152\u001b[0;31m           lambda *arrays: np.stack(arrays), *time_steps)\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_unstack_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36mfast_map_structure\u001b[0;34m(func, *structure)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m       return nest_utils.fast_map_structure(\n\u001b[0;32m--> 152\u001b[0;31m           lambda *arrays: np.stack(arrays), *time_steps)\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_unstack_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mc['k'] = 8\n",
    "mc['v_net'] = 'shared'\n",
    "runname = 'fc:128_%s_k:%d_noisy_st:36' % (mc['v_net'], mc['k'])\n",
    "learn_pool.run_experiment(runname, [mc], env_config, rewards_config, 5000, \n",
    "                          folder = folder, method = method, nsteps = steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
