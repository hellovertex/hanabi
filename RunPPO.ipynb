{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hanabi_learning_environment import rl_env\n",
    "import numpy as np\n",
    "from PPOAgent.util import *\n",
    "from tf_agents_lib import *\n",
    "from PPOAgent.Model import Model\n",
    "from PPOAgent.Game import Game\n",
    "from PPOAgent.learn import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_type = 'Hanabi-Small'\n",
    "NUM_PLAYERS = 2\n",
    "\n",
    "MODEL_CONFIG = {'nsteps' : 36, 'nminibatches' : 1, 'nenvs'  :32,\n",
    "                'fc_input_layers' : [150-75], \n",
    "                'lstm_layers' : [], 'noisy_fc' :False, 'noisy_lstm' : False,\n",
    "                'v_net' : 'copy', 'gamma' : 0.99,\n",
    "                'ent_coef' : 0.0, 'vf_coef' : 1,\n",
    "                'lr' : 1e-3, 'masked' : True,\n",
    "                'max_grad_norm' : None,\n",
    "                'total_timesteps' : int(6500e6),\n",
    "                'normalize_advs': True,\n",
    "                'layer_norm' : False,\n",
    "                 'scope' : 'openhandedagent' }\n",
    "\n",
    "\n",
    "\n",
    "REWARDS_CONFIG = {'play0' : 1, 'play1' : 3, 'play2' : 9, 'play3' : 27, 'play4' : 81, \n",
    "                  'baseline' :3,\n",
    "                  'discard_last_copy' : -100, 'discard_extra' : 0.5, \n",
    "                  'hint_last_copy' : 0.2, 'hint_penalty' : 0.1,  'hint_playable' : 0.2,\n",
    "                  'use_hamming' : False, 'loose_life' : -50}\n",
    "\n",
    "ENV_CONFIG = {'environment_name' : game_type, 'num_players' : NUM_PLAYERS, 'use_custom_rewards' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1125 19:35:20.911845 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:100: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1125 19:35:20.913106 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:101: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1125 19:35:21.036165 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1125 19:35:21.040793 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:38: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1125 19:35:21.046946 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:47: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W1125 19:35:21.049011 140421547276096 deprecation.py:323] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/util.py:299: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOBS: 191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 19:35:21.332331 140421547276096 deprecation.py:323] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/network_building.py:143: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "W1125 19:35:21.460128 140421547276096 deprecation.py:323] From /home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1125 19:35:21.603978 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:98: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W1125 19:35:22.951683 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:120: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W1125 19:35:22.954132 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:121: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------UPDATE0----------------\n",
      "R:-39.78 Score: 0.00 Length: 4 Updates/batch: 24\n",
      "1868.3 steps/second\n",
      "2368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 4.577605166051661\n",
      "play_reward -45.40590405904059\n",
      "discard_reward 1.0444341943419433\n",
      "-------------------------------------------\n",
      "-----------------UPDATE50----------------\n",
      "R:-20.35 Score: 0.11 Length: 20 Updates/batch: 24\n",
      "3186.9 steps/second\n",
      "120768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 24.60545267489712\n",
      "play_reward -42.5840877914952\n",
      "discard_reward -6.574965706447188\n",
      "-------------------------------------------\n",
      "-----------------UPDATE100----------------\n",
      "R:-18.03 Score: 0.23 Length: 20 Updates/batch: 24\n",
      "3258.0 steps/second\n",
      "239168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 31.93247605633803\n",
      "play_reward -37.971830985915496\n",
      "discard_reward -13.510167253521127\n",
      "-------------------------------------------\n",
      "-----------------UPDATE150----------------\n",
      "R:-13.89 Score: 0.46 Length: 21 Updates/batch: 24\n",
      "3223.9 steps/second\n",
      "357568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 32.32889817326822\n",
      "play_reward -34.97485982998734\n",
      "discard_reward -13.479110146500272\n",
      "-------------------------------------------\n",
      "-----------------UPDATE200----------------\n",
      "R:-6.30 Score: 0.68 Length: 22 Updates/batch: 24\n",
      "3214.3 steps/second\n",
      "475968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 33.54801701323252\n",
      "play_reward -30.253308128544422\n",
      "discard_reward -13.556238185255198\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 19:38:29.101790 140421547276096 deprecation.py:323] From /home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------UPDATE250----------------\n",
      "R:7.56 Score: 1.28 Length: 24 Updates/batch: 24\n",
      "3224.7 steps/second\n",
      "594368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.818145633053796\n",
      "play_reward -20.45633053794232\n",
      "discard_reward -12.989483193563782\n",
      "-------------------------------------------\n",
      "-----------------UPDATE300----------------\n",
      "R:23.66 Score: 1.80 Length: 25 Updates/batch: 24\n",
      "3220.5 steps/second\n",
      "712768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.092638566912534\n",
      "play_reward -13.482402528977872\n",
      "discard_reward -4.825149279943799\n",
      "-------------------------------------------\n",
      "-----------------UPDATE350----------------\n",
      "R:29.65 Score: 2.01 Length: 25 Updates/batch: 24\n",
      "3144.4 steps/second\n",
      "831168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.91233766787465\n",
      "play_reward -8.037945001065872\n",
      "discard_reward -0.9030590492432318\n",
      "-------------------------------------------\n",
      "-----------------UPDATE400----------------\n",
      "R:31.18 Score: 2.23 Length: 25 Updates/batch: 24\n",
      "3212.4 steps/second\n",
      "949568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 34.80961980363484\n",
      "play_reward -6.814288698558596\n",
      "discard_reward 0.36543416196643685\n",
      "-------------------------------------------\n",
      "-----------------UPDATE450----------------\n",
      "R:35.76 Score: 2.45 Length: 26 Updates/batch: 24\n",
      "3139.9 steps/second\n",
      "1067968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.14446771378709\n",
      "play_reward -1.5772251308900525\n",
      "discard_reward -0.6939172484002328\n",
      "-------------------------------------------\n",
      "-----------------UPDATE500----------------\n",
      "R:36.42 Score: 2.45 Length: 25 Updates/batch: 24\n",
      "3225.1 steps/second\n",
      "1186368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.491552000000006\n",
      "play_reward -0.5669189189189189\n",
      "discard_reward 0.5950270270270268\n",
      "-------------------------------------------\n",
      "-----------------UPDATE550----------------\n",
      "R:38.23 Score: 2.32 Length: 26 Updates/batch: 24\n",
      "3163.9 steps/second\n",
      "1304768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.36107843137254\n",
      "play_reward 3.2058823529411766\n",
      "discard_reward 1.7136994949494948\n",
      "-------------------------------------------\n",
      "-----------------UPDATE600----------------\n",
      "R:41.01 Score: 2.48 Length: 26 Updates/batch: 21\n",
      "3091.1 steps/second\n",
      "1423168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.9504482300885\n",
      "play_reward 3.1276548672566373\n",
      "discard_reward 0.7062315634218287\n",
      "-------------------------------------------\n",
      "-----------------UPDATE650----------------\n",
      "R:47.30 Score: 2.82 Length: 27 Updates/batch: 17\n",
      "3339.3 steps/second\n",
      "1541568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.13848242591316\n",
      "play_reward 7.823799678382724\n",
      "discard_reward 0.4735048625469024\n",
      "-------------------------------------------\n",
      "-----------------UPDATE700----------------\n",
      "R:48.23 Score: 3.01 Length: 27 Updates/batch: 14\n",
      "3597.7 steps/second\n",
      "1659968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.871153722794965\n",
      "play_reward 9.448339060710195\n",
      "discard_reward 1.7487017945780832\n",
      "-------------------------------------------\n",
      "-----------------UPDATE750----------------\n",
      "R:58.06 Score: 3.47 Length: 27 Updates/batch: 12\n",
      "3614.8 steps/second\n",
      "1778368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.79569060513304\n",
      "play_reward 15.603720273133977\n",
      "discard_reward 3.5394396044266543\n",
      "-------------------------------------------\n",
      "-----------------UPDATE800----------------\n",
      "R:69.27 Score: 4.02 Length: 29 Updates/batch: 11\n",
      "3644.1 steps/second\n",
      "1896768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.8543738814994\n",
      "play_reward 21.933010882708587\n",
      "discard_reward 4.469326884320838\n",
      "-------------------------------------------\n",
      "-----------------UPDATE850----------------\n",
      "R:76.31 Score: 4.29 Length: 29 Updates/batch: 10\n",
      "3906.4 steps/second\n",
      "2015168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.9394626647145\n",
      "play_reward 28.215959004392385\n",
      "discard_reward 3.79727916056613\n",
      "-------------------------------------------\n",
      "-----------------UPDATE900----------------\n",
      "R:72.76 Score: 4.28 Length: 29 Updates/batch: 10\n",
      "3458.0 steps/second\n",
      "2133568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.978792360430944\n",
      "play_reward 29.679970617042116\n",
      "discard_reward 3.2040075089781253\n",
      "-------------------------------------------\n",
      "-----------------UPDATE950----------------\n",
      "R:76.15 Score: 4.09 Length: 29 Updates/batch: 11\n",
      "3925.4 steps/second\n",
      "2251968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 39.249447478474785\n",
      "play_reward 31.192619926199264\n",
      "discard_reward 4.575317753177531\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1000----------------\n",
      "R:75.97 Score: 4.20 Length: 28 Updates/batch: 10\n",
      "3630.5 steps/second\n",
      "2370368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 39.3578021760633\n",
      "play_reward 33.090257171117706\n",
      "discard_reward 5.375803659742829\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1050----------------\n",
      "R:78.37 Score: 4.19 Length: 28 Updates/batch: 11\n",
      "3727.6 steps/second\n",
      "2488768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.19924613899614\n",
      "play_reward 32.44280888030888\n",
      "discard_reward 6.594071750321749\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1100----------------\n",
      "R:80.09 Score: 4.27 Length: 28 Updates/batch: 12\n",
      "3911.4 steps/second\n",
      "2607168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.4818893222818\n",
      "play_reward 34.642857142857146\n",
      "discard_reward 6.809361287176987\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1150----------------\n",
      "R:81.23 Score: 4.52 Length: 28 Updates/batch: 11\n",
      "3728.8 steps/second\n",
      "2725568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.863004655721625\n",
      "play_reward 36.17740749816222\n",
      "discard_reward 6.13511802662746\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1200----------------\n",
      "R:86.84 Score: 4.68 Length: 29 Updates/batch: 9\n",
      "3292.9 steps/second\n",
      "2843968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.59157972079353\n",
      "play_reward 37.75728630908645\n",
      "discard_reward 6.729773859090537\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1250----------------\n",
      "R:85.03 Score: 4.37 Length: 28 Updates/batch: 12\n",
      "3653.4 steps/second\n",
      "2962368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.92527761341222\n",
      "play_reward 39.4948224852071\n",
      "discard_reward 6.742336456278763\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1300----------------\n",
      "R:79.46 Score: 3.89 Length: 28 Updates/batch: 11\n",
      "3553.6 steps/second\n",
      "3080768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.675089423076926\n",
      "play_reward 34.01370192307692\n",
      "discard_reward 8.128445512820512\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1350----------------\n",
      "R:77.09 Score: 3.84 Length: 28 Updates/batch: 8\n",
      "3471.6 steps/second\n",
      "3199168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.777513982642226\n",
      "play_reward 34.70009643201543\n",
      "discard_reward 6.377892960462874\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1400----------------\n",
      "R:82.63 Score: 3.92 Length: 28 Updates/batch: 9\n",
      "3806.2 steps/second\n",
      "3317568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.41161963190183\n",
      "play_reward 37.23430863614913\n",
      "discard_reward 8.520744848198836\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1450----------------\n",
      "R:85.44 Score: 4.04 Length: 27 Updates/batch: 9\n",
      "3565.1 steps/second\n",
      "3435968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.27720700260232\n",
      "play_reward 41.50792524248876\n",
      "discard_reward 8.030616670609573\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1500----------------\n",
      "R:89.18 Score: 4.31 Length: 28 Updates/batch: 10\n",
      "3752.7 steps/second\n",
      "3554368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.838976241900646\n",
      "play_reward 45.0247180225582\n",
      "discard_reward 8.032057435405166\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------UPDATE1550----------------\n",
      "R:91.23 Score: 4.17 Length: 28 Updates/batch: 9\n",
      "3653.6 steps/second\n",
      "3672768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.48229772998805\n",
      "play_reward 45.37084826762246\n",
      "discard_reward 7.970031859816805\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1600----------------\n",
      "R:93.55 Score: 4.42 Length: 28 Updates/batch: 10\n",
      "3539.3 steps/second\n",
      "3791168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.970559748427675\n",
      "play_reward 48.010159651669085\n",
      "discard_reward 8.473169650056441\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1650----------------\n",
      "R:96.79 Score: 4.69 Length: 28 Updates/batch: 9\n",
      "3736.6 steps/second\n",
      "3909568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.73863494352319\n",
      "play_reward 48.426339822158134\n",
      "discard_reward 7.6292157333974195\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1700----------------\n",
      "R:90.94 Score: 4.23 Length: 27 Updates/batch: 11\n",
      "3761.2 steps/second\n",
      "4027968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.29977476835352\n",
      "play_reward 47.995723449750535\n",
      "discard_reward 8.225489031440564\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1750----------------\n",
      "R:93.01 Score: 4.05 Length: 28 Updates/batch: 10\n",
      "3560.4 steps/second\n",
      "4146368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.63108745795291\n",
      "play_reward 48.484863046612205\n",
      "discard_reward 8.922553259650808\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1800----------------\n",
      "R:96.41 Score: 4.30 Length: 27 Updates/batch: 11\n",
      "3649.3 steps/second\n",
      "4264768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.872494440501534\n",
      "play_reward 48.33735509817838\n",
      "discard_reward 9.317601135557133\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1850----------------\n",
      "R:97.74 Score: 4.63 Length: 28 Updates/batch: 9\n",
      "3724.1 steps/second\n",
      "4383168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.78492374517374\n",
      "play_reward 50.58011583011583\n",
      "discard_reward 8.125925032175031\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1900----------------\n",
      "R:94.60 Score: 4.34 Length: 28 Updates/batch: 9\n",
      "3844.0 steps/second\n",
      "4501568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.867588731037806\n",
      "play_reward 51.70431013725018\n",
      "discard_reward 7.520527329641223\n",
      "-------------------------------------------\n",
      "-----------------UPDATE1950----------------\n",
      "R:102.62 Score: 4.71 Length: 28 Updates/batch: 10\n",
      "3366.4 steps/second\n",
      "4619968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.65871999999999\n",
      "play_reward 55.129879518072286\n",
      "discard_reward 8.829538152610441\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2000----------------\n",
      "R:100.78 Score: 4.74 Length: 28 Updates/batch: 9\n",
      "3678.7 steps/second\n",
      "4738368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.8399556412729\n",
      "play_reward 54.14585342333655\n",
      "discard_reward 8.665642076502731\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2050----------------\n",
      "R:103.00 Score: 4.55 Length: 28 Updates/batch: 9\n",
      "3733.6 steps/second\n",
      "4856768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.77255020566175\n",
      "play_reward 57.78828937817566\n",
      "discard_reward 9.086861843697072\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2100----------------\n",
      "R:110.91 Score: 5.18 Length: 29 Updates/batch: 9\n",
      "3848.9 steps/second\n",
      "4975168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.90968693966561\n",
      "play_reward 59.67458202083838\n",
      "discard_reward 8.269525886438897\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2150----------------\n",
      "R:105.57 Score: 4.65 Length: 28 Updates/batch: 10\n",
      "3545.7 steps/second\n",
      "5093568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.0031565681047\n",
      "play_reward 60.45758603974794\n",
      "discard_reward 8.067377605428987\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2200----------------\n",
      "R:103.83 Score: 4.37 Length: 28 Updates/batch: 10\n",
      "3861.8 steps/second\n",
      "5211968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.446660483280446\n",
      "play_reward 59.09128630705394\n",
      "discard_reward 7.277621837116588\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2250----------------\n",
      "R:107.26 Score: 4.41 Length: 28 Updates/batch: 10\n",
      "3759.0 steps/second\n",
      "5330368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.489718749999994\n",
      "play_reward 61.66466346153846\n",
      "discard_reward 8.246995192307692\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2300----------------\n",
      "R:102.81 Score: 4.01 Length: 27 Updates/batch: 9\n",
      "3369.6 steps/second\n",
      "5448768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.73886509635974\n",
      "play_reward 60.48037116345468\n",
      "discard_reward 8.74912760726465\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2350----------------\n",
      "R:104.50 Score: 4.31 Length: 28 Updates/batch: 9\n",
      "3494.4 steps/second\n",
      "5567168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.050812977099234\n",
      "play_reward 61.153625954198475\n",
      "discard_reward 8.704039440203562\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2400----------------\n",
      "R:109.16 Score: 4.44 Length: 27 Updates/batch: 10\n",
      "3459.7 steps/second\n",
      "5685568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.968201487167185\n",
      "play_reward 64.95706404413528\n",
      "discard_reward 8.981950107939554\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2450----------------\n",
      "R:114.52 Score: 4.69 Length: 28 Updates/batch: 10\n",
      "3315.0 steps/second\n",
      "5803968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.81442574493445\n",
      "play_reward 65.20858164481525\n",
      "discard_reward 9.33373063170441\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2500----------------\n",
      "R:112.71 Score: 4.66 Length: 28 Updates/batch: 10\n",
      "3898.7 steps/second\n",
      "5922368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.11672200956937\n",
      "play_reward 66.98444976076556\n",
      "discard_reward 9.091108452950557\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2550----------------\n",
      "R:105.94 Score: 4.07 Length: 28 Updates/batch: 11\n",
      "3429.6 steps/second\n",
      "6040768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.38786769304787\n",
      "play_reward 64.4736588886216\n",
      "discard_reward 8.726024376553605\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2600----------------\n",
      "R:110.95 Score: 4.24 Length: 27 Updates/batch: 9\n",
      "3640.1 steps/second\n",
      "6159168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.63838445129178\n",
      "play_reward 67.2860867504148\n",
      "discard_reward 9.641285454689106\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2650----------------\n",
      "R:112.01 Score: 4.07 Length: 28 Updates/batch: 10\n",
      "3869.3 steps/second\n",
      "6277568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.86891956211328\n",
      "play_reward 66.89719181342218\n",
      "discard_reward 8.876328732349673\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2700----------------\n",
      "R:109.95 Score: 3.99 Length: 27 Updates/batch: 10\n",
      "3973.6 steps/second\n",
      "6395968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 34.9530016252612\n",
      "play_reward 63.44207104713257\n",
      "discard_reward 9.441954957046669\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2750----------------\n",
      "R:112.99 Score: 3.86 Length: 28 Updates/batch: 10\n",
      "3580.8 steps/second\n",
      "6514368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.63417527238276\n",
      "play_reward 67.37873045949787\n",
      "discard_reward 9.025876361913785\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2800----------------\n",
      "R:115.12 Score: 4.01 Length: 27 Updates/batch: 9\n",
      "3892.8 steps/second\n",
      "6632768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.24239849092195\n",
      "play_reward 71.00943173779768\n",
      "discard_reward 9.125776153422937\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------UPDATE2850----------------\n",
      "R:120.27 Score: 4.29 Length: 27 Updates/batch: 10\n",
      "3957.4 steps/second\n",
      "6751168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.222145608266786\n",
      "play_reward 69.22357914513856\n",
      "discard_reward 9.000802411147642\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2900----------------\n",
      "R:119.47 Score: 3.92 Length: 28 Updates/batch: 9\n",
      "3942.5 steps/second\n",
      "6869568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.04557085781433\n",
      "play_reward 73.83031727379553\n",
      "discard_reward 9.529788484136308\n",
      "-------------------------------------------\n",
      "-----------------UPDATE2950----------------\n",
      "R:124.36 Score: 4.74 Length: 28 Updates/batch: 9\n",
      "3875.7 steps/second\n",
      "6987968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.21598126801153\n",
      "play_reward 75.5007204610951\n",
      "discard_reward 8.931776336855586\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3000----------------\n",
      "R:120.77 Score: 4.24 Length: 28 Updates/batch: 9\n",
      "3759.3 steps/second\n",
      "7106368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.20350906095552\n",
      "play_reward 76.63379618733819\n",
      "discard_reward 9.028065427159332\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3050----------------\n",
      "R:123.13 Score: 4.26 Length: 28 Updates/batch: 10\n",
      "3925.1 steps/second\n",
      "7224768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.495698104265394\n",
      "play_reward 77.47085308056872\n",
      "discard_reward 9.295142180094787\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3100----------------\n",
      "R:122.24 Score: 4.27 Length: 28 Updates/batch: 9\n",
      "3907.5 steps/second\n",
      "7343168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.442511153298526\n",
      "play_reward 77.83198860939724\n",
      "discard_reward 9.341777408637874\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3150----------------\n",
      "R:124.13 Score: 4.25 Length: 28 Updates/batch: 9\n",
      "3729.4 steps/second\n",
      "7461568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.59988361554972\n",
      "play_reward 81.59193894586215\n",
      "discard_reward 9.067632562206851\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3200----------------\n",
      "R:129.21 Score: 4.27 Length: 28 Updates/batch: 9\n",
      "3948.9 steps/second\n",
      "7579968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.177689025835505\n",
      "play_reward 77.33870585446789\n",
      "discard_reward 9.05196729082721\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3250----------------\n",
      "R:121.98 Score: 4.24 Length: 28 Updates/batch: 10\n",
      "3919.2 steps/second\n",
      "7698368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.94621248213435\n",
      "play_reward 78.94902334444974\n",
      "discard_reward 9.117714784818167\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3300----------------\n",
      "R:118.92 Score: 4.10 Length: 28 Updates/batch: 9\n",
      "3553.7 steps/second\n",
      "7816768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.345857848905624\n",
      "play_reward 79.74111555660156\n",
      "discard_reward 8.85161214403389\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3350----------------\n",
      "R:122.41 Score: 4.43 Length: 28 Updates/batch: 8\n",
      "3895.7 steps/second\n",
      "7935168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.86566491395793\n",
      "play_reward 80.04804015296367\n",
      "discard_reward 7.858648024219247\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3400----------------\n",
      "R:125.43 Score: 4.24 Length: 28 Updates/batch: 8\n",
      "3279.4 steps/second\n",
      "8053568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.40976703191996\n",
      "play_reward 83.40281086231539\n",
      "discard_reward 9.782138319834841\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3450----------------\n",
      "R:132.90 Score: 4.80 Length: 28 Updates/batch: 8\n",
      "3533.1 steps/second\n",
      "8171968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.588144664597756\n",
      "play_reward 81.4390069228933\n",
      "discard_reward 8.878829474019255\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3500----------------\n",
      "R:130.12 Score: 4.87 Length: 28 Updates/batch: 9\n",
      "3886.2 steps/second\n",
      "8290368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.70089780324737\n",
      "play_reward 84.2805635148042\n",
      "discard_reward 8.8841929321872\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3550----------------\n",
      "R:125.59 Score: 4.17 Length: 27 Updates/batch: 9\n",
      "3908.7 steps/second\n",
      "8408768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.136288625592414\n",
      "play_reward 80.10947867298579\n",
      "discard_reward 8.88234597156398\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3600----------------\n",
      "R:123.86 Score: 4.18 Length: 28 Updates/batch: 8\n",
      "3817.3 steps/second\n",
      "8527168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.4797322235434\n",
      "play_reward 81.64898929845423\n",
      "discard_reward 7.982798256044391\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3650----------------\n",
      "R:139.18 Score: 5.06 Length: 28 Updates/batch: 9\n",
      "3965.7 steps/second\n",
      "8645568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.435219138755976\n",
      "play_reward 90.48325358851675\n",
      "discard_reward 8.872886762360446\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3700----------------\n",
      "R:122.70 Score: 4.31 Length: 28 Updates/batch: 9\n",
      "3577.8 steps/second\n",
      "8763968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.19572107388928\n",
      "play_reward 86.85816108339273\n",
      "discard_reward 8.07505741664687\n",
      "-------------------------------------------\n",
      "-----------------UPDATE3750----------------\n",
      "R:133.26 Score: 4.66 Length: 28 Updates/batch: 8\n",
      "3740.9 steps/second\n",
      "8882368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.10444142857143\n",
      "play_reward 89.0104761904762\n",
      "discard_reward 9.541051587301586\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7484a8e2369c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m       \u001b[0mwait_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0mtarget_kl_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_kl_goal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.996\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       root_folder = './experiments/openhanded/', load = False, write_sum = True,)\n\u001b[0m",
      "\u001b[0;32m~/Documents/HLE github/hanabi/PPOAgent/learn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(run_name, nupdates, k, ENV_CONFIG, MODEL_CONFIG, REWARDS_CONFIG, wait_rewards, target_kl_init, target_kl_goal, kl_factor, root_folder, load, write_sum, eval_every)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mlengths_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mstate_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_kl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mtraining_steps_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/PPOAgent/Game.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, k, target_kl)\u001b[0m\n\u001b[1;32m    405\u001b[0m                                                                                      \u001b[0mmb_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                                                                                      \u001b[0mmb_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_states_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                                                                                      mb_legal_moves, mb_noise)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[0mp_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/PPOAgent/Model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(obs, cliprange, neglogps, inp_states, rewards, masks, actions, values, inp_states_v, legal_moves, generated_noise, temp)\u001b[0m\n\u001b[1;32m    133\u001b[0m                                                                                      \u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                                                                                      _train_actor, _train_critic],\n\u001b[0;32m--> 135\u001b[0;31m                                                                                     feed_dict)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn('rewards_as_vaios_150-75', 100000, 24, ENV_CONFIG, MODEL_CONFIG, REWARDS_CONFIG, \n",
    "      wait_rewards = True,\n",
    "      target_kl_init = .35, target_kl_goal = 0.015, kl_factor = 0.996, \n",
    "      root_folder = './experiments/openhanded/', load = False, write_sum = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl_env.make(**ENV_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obs['player_observations'][0]['vectorized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
