{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hanabi_learning_environment import rl_env\n",
    "import numpy as np\n",
    "from PPOAgent.util import *\n",
    "from tf_agents_lib import *\n",
    "from PPOAgent.Model import Model\n",
    "from PPOAgent.Game import Game\n",
    "from PPOAgent.learn import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_type = 'Hanabi-Small'\n",
    "NUM_PLAYERS = 2\n",
    "\n",
    "MODEL_CONFIG = {'nsteps' : 36, 'nminibatches' : 1, 'nenvs'  :32,\n",
    "                'fc_input_layers' : [150-75], \n",
    "                'lstm_layers' : [], 'noisy_fc' :False, 'noisy_lstm' : False,\n",
    "                'v_net' : 'copy', 'gamma' : 0.99,\n",
    "                'ent_coef' : 0.0, 'vf_coef' : 1,\n",
    "                'lr' : 1e-3, 'masked' : True,\n",
    "                'max_grad_norm' : None,\n",
    "                'total_timesteps' : int(6500e6),\n",
    "                'normalize_advs': True,\n",
    "                'layer_norm' : False,\n",
    "                 'scope' : 'openhandedagent' }\n",
    "\n",
    "\n",
    "\n",
    "REWARDS_CONFIG = {'play0' : 1, 'play1' : 3, 'play2' : 9, 'play3' : 27, 'play4' : 81, \n",
    "                  'baseline' :3,\n",
    "                  'discard_last_copy' : -100, 'discard_extra' : 0.5, \n",
    "                  'hint_last_copy' : 0.2, 'hint_penalty' : 0.1,  'hint_playable' : 0.2,\n",
    "                  'use_hamming' : False, 'loose_life' : -50}\n",
    "\n",
    "ENV_CONFIG = {'environment_name' : game_type, 'num_players' : NUM_PLAYERS, 'use_custom_rewards' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1125 19:35:20.911845 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:100: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1125 19:35:20.913106 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:101: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1125 19:35:21.036165 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1125 19:35:21.040793 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:38: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1125 19:35:21.046946 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:47: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W1125 19:35:21.049011 140421547276096 deprecation.py:323] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/util.py:299: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOBS: 191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 19:35:21.332331 140421547276096 deprecation.py:323] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/network_building.py:143: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "W1125 19:35:21.460128 140421547276096 deprecation.py:323] From /home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1125 19:35:21.603978 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/Model.py:98: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W1125 19:35:22.951683 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:120: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W1125 19:35:22.954132 140421547276096 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgent/learn.py:121: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------UPDATE0----------------\n",
      "R:-39.78 Score: 0.00 Length: 4 Updates/batch: 24\n",
      "1868.3 steps/second\n",
      "2368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 4.577605166051661\n",
      "play_reward -45.40590405904059\n",
      "discard_reward 1.0444341943419433\n",
      "-------------------------------------------\n",
      "-----------------UPDATE50----------------\n",
      "R:-20.35 Score: 0.11 Length: 20 Updates/batch: 24\n",
      "3186.9 steps/second\n",
      "120768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 24.60545267489712\n",
      "play_reward -42.5840877914952\n",
      "discard_reward -6.574965706447188\n",
      "-------------------------------------------\n",
      "-----------------UPDATE100----------------\n",
      "R:-18.03 Score: 0.23 Length: 20 Updates/batch: 24\n",
      "3258.0 steps/second\n",
      "239168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 31.93247605633803\n",
      "play_reward -37.971830985915496\n",
      "discard_reward -13.510167253521127\n",
      "-------------------------------------------\n",
      "-----------------UPDATE150----------------\n",
      "R:-13.89 Score: 0.46 Length: 21 Updates/batch: 24\n",
      "3223.9 steps/second\n",
      "357568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 32.32889817326822\n",
      "play_reward -34.97485982998734\n",
      "discard_reward -13.479110146500272\n",
      "-------------------------------------------\n",
      "-----------------UPDATE200----------------\n",
      "R:-6.30 Score: 0.68 Length: 22 Updates/batch: 24\n",
      "3214.3 steps/second\n",
      "475968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 33.54801701323252\n",
      "play_reward -30.253308128544422\n",
      "discard_reward -13.556238185255198\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 19:38:29.101790 140421547276096 deprecation.py:323] From /home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------UPDATE250----------------\n",
      "R:7.56 Score: 1.28 Length: 24 Updates/batch: 24\n",
      "3224.7 steps/second\n",
      "594368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.818145633053796\n",
      "play_reward -20.45633053794232\n",
      "discard_reward -12.989483193563782\n",
      "-------------------------------------------\n",
      "-----------------UPDATE300----------------\n",
      "R:23.66 Score: 1.80 Length: 25 Updates/batch: 24\n",
      "3220.5 steps/second\n",
      "712768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.092638566912534\n",
      "play_reward -13.482402528977872\n",
      "discard_reward -4.825149279943799\n",
      "-------------------------------------------\n",
      "-----------------UPDATE350----------------\n",
      "R:29.65 Score: 2.01 Length: 25 Updates/batch: 24\n",
      "3144.4 steps/second\n",
      "831168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.91233766787465\n",
      "play_reward -8.037945001065872\n",
      "discard_reward -0.9030590492432318\n",
      "-------------------------------------------\n",
      "-----------------UPDATE400----------------\n",
      "R:31.18 Score: 2.23 Length: 25 Updates/batch: 24\n",
      "3212.4 steps/second\n",
      "949568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 34.80961980363484\n",
      "play_reward -6.814288698558596\n",
      "discard_reward 0.36543416196643685\n",
      "-------------------------------------------\n",
      "-----------------UPDATE450----------------\n",
      "R:35.76 Score: 2.45 Length: 26 Updates/batch: 24\n",
      "3139.9 steps/second\n",
      "1067968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.14446771378709\n",
      "play_reward -1.5772251308900525\n",
      "discard_reward -0.6939172484002328\n",
      "-------------------------------------------\n",
      "-----------------UPDATE500----------------\n",
      "R:36.42 Score: 2.45 Length: 25 Updates/batch: 24\n",
      "3225.1 steps/second\n",
      "1186368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.491552000000006\n",
      "play_reward -0.5669189189189189\n",
      "discard_reward 0.5950270270270268\n",
      "-------------------------------------------\n",
      "-----------------UPDATE550----------------\n",
      "R:38.23 Score: 2.32 Length: 26 Updates/batch: 24\n",
      "3163.9 steps/second\n",
      "1304768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.36107843137254\n",
      "play_reward 3.2058823529411766\n",
      "discard_reward 1.7136994949494948\n",
      "-------------------------------------------\n",
      "-----------------UPDATE600----------------\n",
      "R:41.01 Score: 2.48 Length: 26 Updates/batch: 21\n",
      "3091.1 steps/second\n",
      "1423168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 35.9504482300885\n",
      "play_reward 3.1276548672566373\n",
      "discard_reward 0.7062315634218287\n",
      "-------------------------------------------\n",
      "-----------------UPDATE650----------------\n",
      "R:47.30 Score: 2.82 Length: 27 Updates/batch: 17\n",
      "3339.3 steps/second\n",
      "1541568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.13848242591316\n",
      "play_reward 7.823799678382724\n",
      "discard_reward 0.4735048625469024\n",
      "-------------------------------------------\n",
      "-----------------UPDATE700----------------\n",
      "R:48.23 Score: 3.01 Length: 27 Updates/batch: 14\n",
      "3597.7 steps/second\n",
      "1659968 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 36.871153722794965\n",
      "play_reward 9.448339060710195\n",
      "discard_reward 1.7487017945780832\n",
      "-------------------------------------------\n",
      "-----------------UPDATE750----------------\n",
      "R:58.06 Score: 3.47 Length: 27 Updates/batch: 12\n",
      "3614.8 steps/second\n",
      "1778368 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 37.79569060513304\n",
      "play_reward 15.603720273133977\n",
      "discard_reward 3.5394396044266543\n",
      "-------------------------------------------\n",
      "-----------------UPDATE800----------------\n",
      "R:69.27 Score: 4.02 Length: 29 Updates/batch: 11\n",
      "3644.1 steps/second\n",
      "1896768 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.8543738814994\n",
      "play_reward 21.933010882708587\n",
      "discard_reward 4.469326884320838\n",
      "-------------------------------------------\n",
      "-----------------UPDATE850----------------\n",
      "R:76.31 Score: 4.29 Length: 29 Updates/batch: 10\n",
      "3906.4 steps/second\n",
      "2015168 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.9394626647145\n",
      "play_reward 28.215959004392385\n",
      "discard_reward 3.79727916056613\n",
      "-------------------------------------------\n",
      "-----------------UPDATE900----------------\n",
      "R:72.76 Score: 4.28 Length: 29 Updates/batch: 10\n",
      "3458.0 steps/second\n",
      "2133568 steps in total\n",
      "0.03 steps wasted\n",
      "Current learning rate is 0.00100\n",
      "hint_reward 38.978792360430944\n",
      "play_reward 29.679970617042116\n",
      "discard_reward 3.2040075089781253\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learn('rewards_as_vaios_150-75', 100000, 24, ENV_CONFIG, MODEL_CONFIG, REWARDS_CONFIG, \n",
    "      wait_rewards = True,\n",
    "      target_kl_init = .35, target_kl_goal = 0.015, kl_factor = 0.996, \n",
    "      root_folder = './experiments/openhanded/', load = False, write_sum = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl_env.make(**ENV_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obs['player_observations'][0]['vectorized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
