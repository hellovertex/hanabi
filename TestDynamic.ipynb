{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "from collections import defaultdict\n",
    "def check_create_path(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def make_load_hanabi_fn(ENV_CONFIG):\n",
    "    return lambda: PyhanabiEnvWrapper(rl_env.make(**ENV_CONFIG))\n",
    "\n",
    "def get_env_spec(ENV_CONFIG,):\n",
    "    env = PyhanabiEnvWrapper(rl_env.make(**ENV_CONFIG))\n",
    "    action_spec = env.action_spec()\n",
    "    obs_spec = env.observation_spec()\n",
    "    nactions = action_spec.maximum + 1 - action_spec.minimum\n",
    "    nobs = obs_spec['state'].shape[0]\n",
    "    nplayers = ENV_CONFIG['num_players']\n",
    "    \n",
    "    print('GAME PARAMETERS: \\n observation length = %d \\n number of actions = %d, number of players = %d ' %\n",
    "          (nobs, nactions, nplayers))\n",
    "    return nobs, nactions, nplayers\n",
    "\n",
    "\n",
    "def create_savers_and_summaries(path, models, ENV_CONFIG):\n",
    "    check_create_path(path)\n",
    "    savers = []\n",
    "    summary_writers = []\n",
    "    for model in models:\n",
    "        \n",
    "        model_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = model.scope)\n",
    "        savers.append(tf.train.Saver(var_list = model_params))\n",
    "        \n",
    "        check_create_path(path + model.scope + '/summary/' )\n",
    "        summary_writers.append(tf.summary.FileWriter(path+ model.scope + '/summary/' ))\n",
    "        \n",
    "    return savers, summary_writers\n",
    "\n",
    "\n",
    "def save_models(models, savers, path, kl_th):\n",
    "    for model, saver in zip(models, savers):\n",
    "        saver.save(model.sess, path + model.scope + '/model/model.cptk')\n",
    "        lr = model.lr.value()\n",
    "        params_dict = {'lr' : lr, 'kl_th' : kl_th}\n",
    "        with open(path + model.scope + '/params_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(params_dict, f)\n",
    "\n",
    "        \n",
    "        \n",
    "def load_models(path, savers, models, kl_init, lr_init):\n",
    "    updates_per_model = []\n",
    "    kl_th_list = []\n",
    "    for saver, model in zip(savers, models):\n",
    "        ckpt = tf.train.get_checkpoint_state(path + model.scope + '/model/')\n",
    "        if ckpt is None:\n",
    "            print('Could not load model %s' % model.scope)\n",
    "            init_num_upd = 0\n",
    "            updates_per_model.append(int(init_num_upd))\n",
    "            kl_th_list.append(kl_init)\n",
    "        else:\n",
    "            saver.restore(model.sess, ckpt.model_checkpoint_path)\n",
    "            init_num_upd = model.sess.run(model.updates)\n",
    "            try:\n",
    "                with open(path + model.scope + '/params_dict.pkl', 'rb') as f:\n",
    "                    params_dict = pickle.load(f)\n",
    "            except:\n",
    "                print('No params dict at', path + model.scope)\n",
    "                params_dict = {'lr' : lr_init, 'kl_th' : kl_init}\n",
    "                print(params_dict)\n",
    "            model.change_lr(params_dict['lr'])\n",
    "            kl_th = params_dict['kl_th']\n",
    "            kl_th_list.append(kl_th)\n",
    "            print('Successfully loaded model %s trained for %d updates' % (model.scope,init_num_upd))\n",
    "            updates_per_model.append(int(init_num_upd))\n",
    "    return kl_th_list     \n",
    "                  \n",
    "def save_summary(models, history_buffers, summary_writers, speed):\n",
    "    for model, buffer, writer in zip(models, history_buffers, summary_writers):\n",
    "        summary = tf.Summary()\n",
    "        for key in buffer:\n",
    "            summary.value.add(tag = key, simple_value = np.nanmean(buffer[key]))\n",
    "            #print(key, buffer[key])\n",
    "        \n",
    "        summary.value.add(tag = 'Perf/ts per sec', simple_value = speed)\n",
    "        model_steps = model.sess.run(model.total_steps)\n",
    "        writer.add_summary(summary, model_steps)\n",
    "        writer.flush()\n",
    "        buffer = defaultdict(list)\n",
    "def print_learning_rates(models):\n",
    "    for m in models:\n",
    "        lr = m.lr.value()\n",
    "        print('Model %s has lr %.5f' % (m.scope, lr))\n",
    "    \n",
    "def write_into_buffer(buffer, training_stats, policy_loss, value_loss, policy_entropy, updates):\n",
    "    # writes all data into buffer dict\n",
    "    buffer['Perf/Score'].append(np.mean(training_stats['scores']))\n",
    "    buffer['Perf/Reward'].append(np.mean(training_stats['rewards']))\n",
    "    buffer['Perf/Length'].append(np.mean(training_stats['lengths']))\n",
    "    buffer['Perf/Reward by \"play\"'].append(np.mean(training_stats['play_reward']))\n",
    "    buffer['Perf/Reward by \"discard\"'].append(np.mean(training_stats['discard_reward']))\n",
    "    buffer['Perf/Reward by \"hint\"'].append(np.mean(training_stats['hint_reward']))\n",
    "    #buffer['Perf/Updates per batch'].append(k_trained)\n",
    "    buffer['Perf/Updates done'].append(updates)\n",
    "    #buffer['Losses/KL loss'].append(np.mean(kl))\n",
    "    buffer['Losses/Policy loss'].append(np.mean(policy_loss))\n",
    "    buffer['Losses/Value loss'].append(np.mean(value_loss))\n",
    "    buffer['Losses/Policy entropy'].append(np.mean(policy_entropy))\n",
    "    \n",
    "\n",
    "def train_one_epoch(game, models, history_buffers, k, kl_th_list, episodes = 90, nsteps = None,\n",
    "                    method = 'self play',\n",
    "                    cliprange = 0.2, epochs_for_ma = 5,):\n",
    "    # methods:\n",
    "    #    self play -- each model will play with its copy, learning \n",
    "    #    multi agent -- each model will play with all other models\n",
    "    #    if train is True all games will be used for training\n",
    "\n",
    "    if method == 'single agent':\n",
    "        for model, model_buffer, kl_th in zip(models, history_buffers, kl_th_list):\n",
    "            # for sigle agent game uses one model\n",
    "            game.players[0].assign_model(model)\n",
    "            #game.reset()\n",
    "            # run game untill enough data to training\n",
    "            training_stats = game.play_untill_train(episodes, nsteps)\n",
    "            # train\n",
    "            policy_loss, value_loss, policy_entropy, k_trained = train_model(game, model, k, kl_th, \n",
    "                                                                                cliprange = cliprange)\n",
    "            updates = model.sess.run(model.updates)\n",
    "            # store results\n",
    "            write_into_buffer(model_buffer, training_stats,\n",
    "                              policy_loss, value_loss, policy_entropy, updates)\n",
    "        \n",
    "        \n",
    "    elif method == 'self play':\n",
    "        for model, model_buffer, kl_th in zip(models, history_buffers, kl_th_list):\n",
    "            for player in game.players:\n",
    "                player.assign_model(model)\n",
    "            \n",
    "            #game.reset()\n",
    "            # run game untill enough data to training\n",
    "            training_stats = game.play_untill_train(episodes, nsteps)\n",
    "            # train\n",
    "            policy_loss, value_loss, policy_entropy, k_trained = train_model(game, model, k, kl_th,\n",
    "                                                                                cliprange = cliprange)\n",
    "            updates = model.sess.run(model.updates)\n",
    "            # store results\n",
    "            write_into_buffer(model_buffer, training_stats,\n",
    "                              policy_loss, value_loss, policy_entropy, updates)\n",
    "            \n",
    "    elif method == 'multi agent':\n",
    "        model_nums = list(range(len(models)))\n",
    "        player_nums = list(range(game.nplayers))\n",
    "        # main_model_num is number of the model which will train by playing with\n",
    "        # other models (including itself) are frozen.\n",
    "        # it will play from position of main_player_num\n",
    "        for main_model_num, model_buffer, kl_th in zip(model_nums, history_buffers, kl_th):\n",
    "            main_model = models[main_model_num]\n",
    "            main_player = random.choice(game.players)\n",
    "            main_player.assign_model(main_model)\n",
    "            other_players = [p for p in game.players if p.num != main_player.num]\n",
    "            for _ in range(epochs_for_ma):\n",
    "                self_play_case = True\n",
    "                for p in other_players:\n",
    "                    p_model = random.choice(models)\n",
    "                    self_play_case = self_play_case and (p_model.scope == main_model.scope)\n",
    "                    p.assign_model(p_model)\n",
    "\n",
    "                #game.reset()\n",
    "                # run game untill enough data to training\n",
    "                training_stats = game.play_untill_train(episodes, nsteps)\n",
    "                # train\n",
    "                if self_play_case:\n",
    "                    train_players = 'all'\n",
    "                else:\n",
    "                    train_players = [main_player.num]\n",
    "                \n",
    "                policy_loss, value_loss, policy_entropy, k_trained = train_model(game, main_model, k, \n",
    "                                                                                 kl_th,\n",
    "                                                                                 train_players, cliprange)\n",
    "                updates = model.sess.run(model.updates)\n",
    "                # store results\n",
    "                write_into_buffer(model_buffer, training_stats,\n",
    "                                  policy_loss, value_loss, policy_entropy, updates)\n",
    "    \n",
    "\n",
    "        \n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PPOAgentDynamic.Model import Model\n",
    "from PPOAgentDynamic.Game import Game\n",
    "from PPOAgentDynamic.learn import learn\n",
    "\n",
    "from tf_agents_lib.pyhanabi_env_wrapper import PyhanabiEnvWrapper\n",
    "from hanabi_learning_environment import rl_env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def train_model(game, model, k, target_kl, player_nums = 'all', cliprange = 0.2):\n",
    "    (mb_obs, mb_actions, mb_probs, mb_neglogps, mb_legal_moves, mb_values, mb_rewards,\n",
    "     mb_dones, mb_noise) = game.collect_data(player_nums)\n",
    "    update_kl = False\n",
    "    for i in range(k):\n",
    "        p_losses, v_losses, p_ents, kl_losses = [], [], [], []\n",
    "        policy_loss, value_loss, policy_entropy, _, probs = model.train(mb_obs, cliprange,\n",
    "                                                                              mb_neglogps, mb_probs,\n",
    "                                                                              mb_rewards, mb_actions,\n",
    "                                                                              mb_values, mb_legal_moves,\n",
    "                                                                              mb_noise, update_kl)\n",
    "                    \n",
    "        p_losses.append(policy_loss)\n",
    "        v_losses.append(value_loss)\n",
    "        p_ents.append(policy_entropy)\n",
    "        #kl_losses.append(kl_loss)\n",
    "\n",
    "    model.sess.run(model.increment_updates)\n",
    "    return np.mean(p_losses), np.mean(v_losses), np.mean(p_ents), i + 1  \n",
    "\n",
    "def run_evaluation(models, game, nepisodes = 10):\n",
    "    # support only 2 players now\n",
    "    result_matrix = -np.ones((len(models), len(models)))\n",
    "    model_nums = list(range(len(models)))\n",
    "    for p1_model_num in model_nums:\n",
    "        \n",
    "        game.players[0].assign_model(models[p1_model_num])\n",
    "        if game.nplayers == 2:\n",
    "            for p2_model_num in model_nums:\n",
    "                game.players[1].assign_model(models[p1_model_num])\n",
    "                game.reset()\n",
    "                result_scores = np.mean(game.eval_results( episodes_per_env = nepisodes)['scores'])\n",
    "                result_matrix[p1_model_num, p2_model_num] = result_scores\n",
    "        else:\n",
    "            game.reset()\n",
    "            result_scores = np.mean(game.eval_results( episodes_per_env = nepisodes)['scores'])\n",
    "            result_matrix[p1_model_num, p1_model_num] = result_scores\n",
    "\n",
    "    return result_matrix\n",
    "\n",
    "def run_experiment(run_name, MODEL_CONFIGS, ENV_CONFIG,  REWARDS_CONFIG,\n",
    "                   nupdates = 10000,  k = 24,  episodes = 90, nsteps = None,\n",
    "                   kl_init = 0.4, target_kl = 0.01, kl_factor = 0.996, cliprange = 0.2,\n",
    "                   save_every = 100, summary_every = 10, evaluation_every = 200, eval_eps = 10,\n",
    "                   root_folder = './experiments/newframework/', method = 'self play'\n",
    "                  ):\n",
    "    # set session\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    # setting up environment depending variables\n",
    "    load_env_fn = make_load_hanabi_fn(ENV_CONFIG)\n",
    "    env = load_env_fn()\n",
    "    nobs, nactions, nplayers = get_env_spec(ENV_CONFIG)\n",
    "    nenvs = MODEL_CONFIGS[0]['nenvs']\n",
    "    path = root_folder + ENV_CONFIG['environment_name'] + '-' + str(ENV_CONFIG['num_players'])\n",
    "    path += '/' + run_name + '/'\n",
    "    # create model\n",
    "    models = [Model(nactions, nobs, nplayers, sess = sess, **MC) for MC in MODEL_CONFIGS]\n",
    "    # create game\n",
    "    if method == 'single agent':\n",
    "        game = Game(1, nenvs, load_env_fn)\n",
    "    else:\n",
    "        game  = Game(nplayers, nenvs, load_env_fn, wait_rewards = True)\n",
    "        \n",
    "    game.reset(REWARDS_CONFIG)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # make savers, summaries, history buffers\n",
    "    savers, summary_writers = create_savers_and_summaries(path, models, ENV_CONFIG)\n",
    "    history_buffers = [defaultdict(list) for _ in range(len(models))]\n",
    "    # try to load models\n",
    "    # run training\n",
    "    #kl_th_list = kl_init * np.ones(len(models))\n",
    "    kl_th_list = np.array(load_models(path, savers, models, kl_init,  MODEL_CONFIGS[0]['lr']))\n",
    "    steps_start, time_start = game.total_steps, time.time()\n",
    "    for nupd in range(1, nupdates):\n",
    "        \n",
    "        # trains each model once\n",
    "        train_one_epoch(game, models, history_buffers, k, kl_th_list, episodes, nsteps, \n",
    "                        cliprange = cliprange)\n",
    "        kl_th_list = np.array([max(kl_factor*kl_th, target_kl) for kl_th in kl_th_list])\n",
    "        # save models once in a while\n",
    "        if nupd % save_every == 0:\n",
    "            print_learning_rates(models)\n",
    "            save_models(models, savers, path, kl_th_list)\n",
    "        # save summaries, more often than models\n",
    "        if nupd % summary_every == 0:\n",
    "            speed = (game.total_steps - steps_start) / (time.time() - time_start)\n",
    "            print('Speed = %d ts/second' % (speed))\n",
    "            steps_start, time_start = game.total_steps, time.time()\n",
    "            save_summary(models, history_buffers, summary_writers, speed)\n",
    "            history_buffers = [defaultdict(list) for _ in range(len(models))]\n",
    "        if nupd % evaluation_every == 0:\n",
    "            matrix = run_evaluation(models, game, eval_eps)\n",
    "            print('---------------%d---------------' % nupd)\n",
    "            print('Matrix of models performance with each others:')\n",
    "            print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REWARDS_CONFIG = {'play0' : 1, 'play1' : 3, 'play2' : 9, 'play3' : 27, 'play4' : 81,\n",
    "                  'baseline' :3,\n",
    "                  'discard_last_copy' : -100, 'discard_extra' : 0.5,\n",
    "                  'hint_last_copy' : 0.2, 'hint_penalty' : 0.1,  'hint_playable' : 0.2,\n",
    "                  'use_hamming' : True, 'loose_life' : -50}\n",
    "\n",
    "ENV_CONFIG = {'environment_name' : 'Hanabi-Small', 'num_players' : 2, \n",
    "              'use_custom_rewards' : True, 'open_hands' : True}\n",
    "\n",
    "MC_BASE = {'nenvs'  : 32, 'fc_input_layers' : [128], \n",
    "           'noisy_fc' : False,\n",
    "           'v_net' : 'copy', 'gamma' : 1,\n",
    "           'ent_coef' : 0.0, 'vf_coef' : 1,\n",
    "           'lr' : 8e-5, 'masked' : True,\n",
    "           'max_grad_norm' : None,\n",
    "           'total_timesteps' : int(100000),\n",
    "           'normalize_advs': True,\n",
    "           'normalize_rewards' : True,\n",
    "           'layer_norm' : False,\n",
    "           'lrschedule' : 'constant',\n",
    "           'scope' : 'agent', \n",
    "           'use_kl_penalty' : False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1129 01:48:21.580241 140436059715392 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/Model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1129 01:48:21.588923 140436059715392 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/Model.py:43: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1129 01:48:21.593518 140436059715392 deprecation.py:506] From /home/gr1/.local/lib/python3.6/site-packages/tf_agents/utils/common.py:147: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1129 01:48:21.624544 140436059715392 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/Model.py:57: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W1129 01:48:21.626341 140436059715392 deprecation.py:323] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/util.py:299: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME PARAMETERS: \n",
      " observation length = 191 \n",
      " number of actions = 11, number of players = 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1129 01:48:21.888865 140436059715392 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/util.py:268: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1129 01:48:22.113499 140436059715392 deprecation.py:323] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/network_building.py:72: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "W1129 01:48:22.131980 140436059715392 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/Model.py:79: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1129 01:48:22.196376 140436059715392 deprecation.py:323] From /home/gr1/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1129 01:48:22.369419 140436059715392 deprecation_wrapper.py:119] From /home/gr1/Documents/HLE github/hanabi/PPOAgentDynamic/Model.py:116: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load model agent\n",
      "Speed = 2683 ts/second\n",
      "Speed = 2846 ts/second\n",
      "Speed = 2874 ts/second\n",
      "Speed = 2610 ts/second\n",
      "Speed = 2630 ts/second\n",
      "Speed = 2620 ts/second\n",
      "---------------250---------------\n",
      "Matrix of models performance with each others:\n",
      "[[0.00729167]]\n",
      "Speed = 2804 ts/second\n",
      "Speed = 2622 ts/second\n",
      "Speed = 2597 ts/second\n",
      "Speed = 2668 ts/second\n",
      "Speed = 2568 ts/second\n",
      "Speed = 2589 ts/second\n",
      "Model agent has lr 0.00008\n",
      "---------------500---------------\n",
      "Matrix of models performance with each others:\n",
      "[[0.31145833]]\n",
      "Speed = 2687 ts/second\n",
      "Speed = 2640 ts/second\n",
      "Speed = 2812 ts/second\n",
      "Speed = 2659 ts/second\n",
      "Speed = 2624 ts/second\n",
      "Speed = 2639 ts/second\n",
      "---------------750---------------\n",
      "Matrix of models performance with each others:\n",
      "[[0.88541667]]\n",
      "Speed = 2710 ts/second\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4be41c20cd7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mkl_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcliprange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0msave_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                method = 'self play')\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nsteps_36'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-013e9f36c076>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(run_name, MODEL_CONFIGS, ENV_CONFIG, REWARDS_CONFIG, nupdates, k, episodes, nsteps, kl_init, target_kl, kl_factor, cliprange, save_every, summary_every, evaluation_every, eval_eps, root_folder, method)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# trains each model once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         train_one_epoch(game, models, history_buffers, k, kl_th_list, episodes, nsteps, \n\u001b[0;32m---> 83\u001b[0;31m                         cliprange = cliprange)\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mkl_th_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl_factor\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkl_th\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_kl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkl_th\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkl_th_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# save models once in a while\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f19d166a5dc6>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(game, models, history_buffers, k, kl_th_list, episodes, nsteps, method, cliprange, epochs_for_ma)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m#game.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# run game untill enough data to training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtraining_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_untill_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             policy_loss, value_loss, policy_entropy, k_trained = train_model(game, model, k, kl_th,\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/PPOAgentDynamic/Game.py\u001b[0m in \u001b[0;36mplay_untill_train\u001b[0;34m(self, nepisodes, nsteps, noisescale)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0msteps_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnsteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_done\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/PPOAgentDynamic/Game.py\u001b[0m in \u001b[0;36mplay_turn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m#print(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_player\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_timestep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    172\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    130\u001b[0m     time_steps = [\n\u001b[1;32m    131\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         for env, action in zip(self._envs, self._unstack_actions(actions))]\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;31m# When blocking is False we get promises that need to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     time_steps = [\n\u001b[1;32m    131\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         for env, action in zip(self._envs, self._unstack_actions(actions))]\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;31m# When blocking is False we get promises that need to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, blocking)\u001b[0m\n\u001b[1;32m    299\u001b[0m       \u001b[0mtime\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0mcallable\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mpromise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mpromise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLE github/hanabi/tf_agents_lib/parallel_py_environment.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \"\"\"\n\u001b[1;32m    277\u001b[0m     \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mc =  dict(MC_BASE)\n",
    "run_name = 'neps_90_k=24'\n",
    "run_experiment(run_name, [mc], ENV_CONFIG,  REWARDS_CONFIG,\n",
    "               nupdates = 2000, k = 24, episodes = 90, nsteps = None,\n",
    "               kl_init = 1, target_kl = 1, kl_factor = 1, cliprange = 0.2,\n",
    "               save_every = 500, summary_every = 40, evaluation_every = 250, eval_eps = 30,\n",
    "               method = 'self play')\n",
    "\n",
    "run_name = 'nsteps_36'\n",
    "run_experiment(run_name, [mc], ENV_CONFIG,  REWARDS_CONFIG,\n",
    "               nupdates = 2200, k = 24, episodes = 90, nsteps = 36,\n",
    "               kl_init = 1, target_kl = 1, kl_factor = 1, cliprange = 0.2,\n",
    "               save_every = 100, summary_every = 10, evaluation_every = 250, eval_eps = 30,\n",
    "               method = 'self play')\n",
    "\n",
    "\n",
    "\n",
    "'''run_name = 'singlagent_oh'\n",
    "run_experiment(run_name, [mc], ENV_CONFIG,  REWARDS_CONFIG,\n",
    "               nupdates = 6000, k = 24, episodes = 90, \n",
    "               kl_init = 1, target_kl = 1, kl_factor = 1, \n",
    "               save_every = 100, summary_every = 10, evaluation_every = 250, eval_eps = 30,\n",
    "               method = 'single agent')\n",
    "\n",
    "\n",
    "run_name = 'selfplay_norm'\n",
    "ENV_CONFIG['open_hands'] = False\n",
    "run_experiment(run_name, [mc], ENV_CONFIG,  REWARDS_CONFIG,\n",
    "               nupdates = 20000, k = 24, episodes = 90, \n",
    "               kl_init = 1, target_kl = 1, kl_factor = 1, \n",
    "               save_every = 100, summary_every = 10, evaluation_every = 250, eval_eps = 30,\n",
    "               method = 'self play')'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_name = 'nsteps_36'\n",
    "run_experiment(run_name, [mc], ENV_CONFIG,  REWARDS_CONFIG,\n",
    "               nupdates = 2200, k = 24, episodes = 90, nsteps = 36,\n",
    "               kl_init = 1, target_kl = 1, kl_factor = 1, cliprange = 0.2,\n",
    "               save_every = 100, summary_every = 10, evaluation_every = 250, eval_eps = 30,\n",
    "               method = 'self play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'nepisodes_90'\n",
    "run_experiment(run_name, [mc], ENV_CONFIG,  REWARDS_CONFIG,\n",
    "               nupdates = 2200, k = 24, episodes = 90, nsteps = None,\n",
    "               kl_init = 1, target_kl = 1, kl_factor = 1, cliprange = 0.2,\n",
    "               save_every = 100, summary_every = 10, evaluation_every = 250, eval_eps = 30,\n",
    "               method = 'self play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
